{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Format\n",
    "This file formats the dataset for hand pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fnmatch\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finger and joints\n",
    "fingers = [\"thumb\", \"index\", \"middle\", \"ring\", \"pinky\"]\n",
    "joints = [\"distal\", \"intermediate\", \"proximal\", \"metacarpal\"]\n",
    "\n",
    "# sequense length\n",
    "n = 10\n",
    "\n",
    "# define which video to use as test case\n",
    "min_value = 0\n",
    "max_value = 20\n",
    "num_integers = 5\n",
    "\n",
    "test = np.random.choice(np.arange(min_value, max_value), num_integers, replace=False)\n",
    "test.sort()\n",
    "\n",
    "# the path to the data\n",
    "root = \"../../../datasets/everyone/data/\"\n",
    "\n",
    "# all the video data folder\n",
    "videos = os.listdir(root)\n",
    "\n",
    "# export location\n",
    "export_location = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "for formatting data annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAnnotation(hand):\n",
    "    an = []\n",
    "    re_an = []\n",
    "    root = np.array(hand[\"palm_position\"])\n",
    "    an.append(hand[\"palm_position\"])\n",
    "    re_an.append([0., 0., 0.])\n",
    "    for finger in fingers:\n",
    "        for joint in joints:\n",
    "            an.append(hand[finger][joint])\n",
    "            re_an.append((np.array(hand[finger][joint]) - root).tolist())\n",
    "    \n",
    "    return an, re_an\n",
    "\n",
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "def recursive_glob(rootdir=\".\", pattern=\"*\"):\n",
    "    matches = []\n",
    "    for root, dirnames, filenames in os.walk(rootdir):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            matches.append(os.path.join(root, filename))\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [f\"data{i}\" for i in test]\n",
    "\n",
    "image_paths = []\n",
    "reletive_hands = []\n",
    "movements = []\n",
    "\n",
    "image_paths_test = []\n",
    "reletive_hands_test = []\n",
    "movements_test = []\n",
    "\n",
    "for video in videos:\n",
    "    if os.path.isdir(os.path.join(root, video)) and \"data\" in video:\n",
    "        colorFrames = recursive_glob(os.path.join(root, video, \"ring\"), \"image[0-9]*\")\n",
    "        colorFrames = natural_sort(colorFrames)\n",
    "        with open(os.path.join(root, video, \"data_leap.json\"), \"r\") as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        if video in test_data:\n",
    "            for i in range(0, len(colorFrames), n):\n",
    "                root_num = str(int(colorFrames[i].split('.')[6].split(\"image\")[-1]) + (n - 2))\n",
    "                root_pos = np.array(readAnnotation(annotation[root_num])[0][0])\n",
    "                image_path = []\n",
    "                reletive_hand = []\n",
    "                movement = []\n",
    "                for j in range(i, i+n):\n",
    "                    num = colorFrames[j].split(\".\")[6].split(\"image\")[-1]\n",
    "                    image_path.append(colorFrames[j])\n",
    "                    _an, re_an = readAnnotation(annotation[num])\n",
    "                    reletive_hand.append(re_an)\n",
    "                    an = []\n",
    "                    for a in _an:\n",
    "                        an.append((np.array(a) - root_pos).tolist())\n",
    "                    movement.append(an)\n",
    "                image_paths_test.append(image_path)\n",
    "                reletive_hands_test.append(reletive_hand)\n",
    "                movements_test.append(movement)\n",
    "        else:\n",
    "            for i in range(0, len(colorFrames), n):\n",
    "                root_num = str(int(colorFrames[i].split('.')[6].split(\"image\")[-1]) + (n - 2))\n",
    "                root_pos = np.array(readAnnotation(annotation[root_num])[0][0])\n",
    "                image_path = []\n",
    "                reletive_hand = []\n",
    "                movement = []\n",
    "                for j in range(i, i+n):\n",
    "                    num = colorFrames[j].split(\".\")[6].split(\"image\")[-1]\n",
    "                    image_path.append(colorFrames[j])\n",
    "                    _an, re_an = readAnnotation(annotation[num])\n",
    "                    reletive_hand.append(re_an)\n",
    "                    an = []\n",
    "                    for a in _an:\n",
    "                        an.append((np.array(a) - root_pos).tolist())\n",
    "                    movement.append(an)\n",
    "                image_paths.append(image_path)\n",
    "                reletive_hands.append(reletive_hand)\n",
    "                movements.append(movement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conver into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = np.array(image_paths)\n",
    "reletive_hands = np.array(reletive_hands)\n",
    "movements = np.array(movements)\n",
    "\n",
    "image_paths_test = np.array(image_paths_test)\n",
    "reletive_hands_test = np.array(reletive_hands_test)\n",
    "movements_test = np.array(movements_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(export_location + \"image_path.npy\", image_paths)\n",
    "np.save(export_location + \"reletive_hands\", reletive_hands)\n",
    "np.save(export_location + \"movement.npy\", movements)\n",
    "\n",
    "np.save(export_location + \"image_path_test.npy\", image_paths_test)\n",
    "np.save(export_location + \"reletive_hands_test\", reletive_hands_test)\n",
    "np.save(export_location + \"movement_test\", movements_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
